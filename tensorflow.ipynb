{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_management import load_data\n",
    "import config as cfg\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"Number of available GPUs : \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On profite de la puissance de Python : Les dictionnaires\n",
    "datasets_map = {\n",
    "    'cifar': None,\n",
    "    'mnist': None,\n",
    "    'fashion': None\n",
    "}\n",
    "\n",
    "for name in datasets_map.keys():\n",
    "    datasets_map[name] = load_data(name, cfg.DATA_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_map['mnist']['train']['images'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_ti = datasets_map['cifar']['train']['images']\n",
    "cifar_tl = datasets_map['cifar']['train']['labels']\n",
    "cifar_shape = cifar_ti.shape\n",
    "\n",
    "cifar_train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    cifar_ti.reshape((cifar_shape[0], cifar_shape[2], cifar_shape[3], cifar_shape[1])),\n",
    "    cifar_tl\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-encodeur\n",
    "\n",
    "Nous commençons par un auto-encodeur en TensorFlow à l'aide d'une classe personnalisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notre bloc encodeur\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_size=8, *args, **kwargs):\n",
    "        super(Encoder, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        # Couches\n",
    "        self.conv1 = layers.Conv2D(filters=4, kernel_size=3, strides=1, padding=\"SAME\", \n",
    "                                   use_bias=True, data_format='channels_last')\n",
    "        self.conv2 = layers.Conv2D(filters=1, kernel_size=3, strides=1, padding=\"SAME\", \n",
    "                                   use_bias=True, data_format='channels_last')\n",
    "        \n",
    "        # Activations\n",
    "        self.relu = layers.ReLU()\n",
    "        \n",
    "        # Couche latente\n",
    "        self.dense = layers.Dense(units=128, use_bias=True)\n",
    "        self.latent = layers.Dense(units=latent_size, use_bias=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.relu(x)\n",
    "        return self.latent(x)\n",
    "\n",
    "# Notre bloc décodeur\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Decoder, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        # Couches\n",
    "        self.de_conv1 = layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding=\"SAME\", \n",
    "                                               use_bias=True, data_format='channels_last')\n",
    "        self.de_conv2 = layers.Conv2DTranspose(filters=4, kernel_size=3, strides=1, padding=\"SAME\", \n",
    "                                               use_bias=True, data_format='channels_last')\n",
    "        \n",
    "        # Activations\n",
    "        self.relu = layers.ReLU()\n",
    "        \n",
    "        # Couche latente\n",
    "        self.dense = layers.Dense(units=784, use_bias=True)\n",
    "        self.de_latent = layers.Dense(units=128, use_bias=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.de_latent(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.de_conv2(x)\n",
    "        x = self.relu(x)\n",
    "        return self.de_conv1(x)\n",
    "\n",
    "\n",
    "# Classe finale : combinaison encodeur - decodeur\n",
    "class ConvolutionalAE(tf.keras.Model):\n",
    "    def __init__(self, latent_size=8, *args, **kwargs):\n",
    "        super(ConvolutionalAE, self).__init__(*args, **kwargs)\n",
    "        self.encoder = Encoder(latent_size=latent_size)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "model = ConvolutionalAE(latent_size=16)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "\n",
    "step_loss = tf.keras.metrics.Mean(name='step_loss')\n",
    "epoch_loss = tf.keras.metrics.Mean(name='epoch_loss')\n",
    "loss_operations = (epoch_loss, step_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_step(model, loss_object, optimizer, loss_operation, inputs, labels=None):\n",
    "    \"\"\"\n",
    "    Fonction qui va rouler une étape, soit le passage d'une batch d'exemples.\n",
    "    De plus, le graph va se construire durant l'exécution à l'aide de tf.GradientTape().\n",
    "    Les labels peuvent être de type 'None', auquel cas, la sortie sera comparée avec l'entrée.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(inputs, training=True)\n",
    "        if labels is None:\n",
    "            loss = loss_object(inputs, outputs)\n",
    "        else:\n",
    "            loss = loss_object(labels, outputs)\n",
    "    # Le gradient se calcule en dehors du contexte du graph\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # La backpropagation\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # Le suivi de la loss\n",
    "    return loss_operation(loss)\n",
    "\n",
    "def train(model, loss_object, optimizer, train_data, loss_operations, epochs=100, use_labels=False):\n",
    "    \"\"\"\n",
    "    Fonction d'entrainement, à chacune des époques, on fait passer l'ensemble du jeu de données.\n",
    "    \"\"\"\n",
    "    epoch_op_loss, step_op_loss = loss_operations\n",
    "    for ep in range(epochs):\n",
    "        for (batch, (inputs, labels)) in enumerate(train_data):\n",
    "            batches_loss = []\n",
    "            if use_labels:\n",
    "                loss = run_step(model, loss_object, optimizer, step_op_loss, inputs, labels)\n",
    "            else:\n",
    "                loss = run_step(model, loss_object, optimizer, step_op_loss, inputs)\n",
    "            batches_loss.append(loss)\n",
    "            print(f'\\rBatch : {batch + 1} / {len(train_data)} - Loss : {loss:0.5f}', end='')\n",
    "        epoch_loss = epoch_op_loss(batches_loss)\n",
    "        print(f'\\rEpoch : {ep+1} / {epochs} - Loss : {epoch_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Finir de loader le dataset\n",
    "train_size = len(cifar_train_dataset)\n",
    "train_dataset = cifar_train_dataset.shuffle(2*train_size).batch(64)\n",
    "train(model, loss, optimizer, train_dataset, loss_operations, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph des opération\n",
    "Je confirme qu'avec **tf.GradientTape()** nous avons bien la construction du graph avec les opérations effectuées durant l'exécution. Le contexte permet d'enregistrer les opération réalisées.\n",
    "\n",
    "Continuer à se renseigner sur le décorateur **@tf.function** : https://www.tensorflow.org/api_docs/python/tf/function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
