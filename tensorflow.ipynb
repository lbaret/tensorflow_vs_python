{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "work_directory = os.path.abspath(os.path.dirname(__name__))\n",
    "data_directory = os.path.join(work_directory, 'data')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-11 23:11:16.344394: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chargement des données"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fonction automatisation du chargement des données"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def load_data(dataset_name: str) -> dict:\n",
    "    # TODO : Ajouter description\n",
    "    data_map = {\n",
    "        'train': {\n",
    "            'images': None,\n",
    "            'labels': None\n",
    "        },\n",
    "        'test': {\n",
    "            'images': None,\n",
    "            'labels': None\n",
    "        }\n",
    "    }\n",
    "    # TODO : Ajouter try-except\n",
    "    for stage in data_map.keys():\n",
    "        for data_type in data_map[stage].keys():\n",
    "            with open(os.path.join(data_directory, f'{dataset_name}_{data_type}_{stage}.npy'), mode='rb') as f:\n",
    "                data_map[stage][data_type] = np.lib.format.read_array(f, allow_pickle=True)\n",
    "    return data_map\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chargement des données"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# On profite de la puissance de Python : Les dictionnaires\n",
    "datasets_map = {\n",
    "    'cifar': None,\n",
    "    'mnist': None,\n",
    "    'fashion': None\n",
    "}\n",
    "\n",
    "for name in datasets_map.keys():\n",
    "    datasets_map[name] = load_data(name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vérification des données chargées\n",
    "\n",
    "Comme notre mapping est légèrement lourd, nous allons juste vérifier si chacun des types soient bien settés à np.ndarray avec une taille > 0."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "verification = True\n",
    "for data_name in datasets_map.keys():\n",
    "    for stage in datasets_map[data_name].keys():\n",
    "        for data_type in datasets_map[data_name][stage].keys():\n",
    "            # On aime les boucles for imbriquées ...\n",
    "            # TODO : Faire en sorte que la vérification soit fait en amont\n",
    "            array = datasets_map[data_name][stage][data_type]\n",
    "            if len(array) == 0 or type(array) is not np.ndarray:\n",
    "                verification = False\n",
    "\n",
    "print(f\"{'Good !' if verification else 'Problem somewhere ...'}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Good !\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}