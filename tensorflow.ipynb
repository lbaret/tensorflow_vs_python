{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-15 21:42:59.398547: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from utils.data_management import load_data\n",
    "import config as cfg\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On profite de la puissance de Python : Les dictionnaires\n",
    "datasets_map = {\n",
    "    'cifar': None,\n",
    "    'mnist': None,\n",
    "    'fashion': None\n",
    "}\n",
    "\n",
    "for name in datasets_map.keys():\n",
    "    datasets_map[name] = load_data(name, cfg.DATA_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_map['mnist']['train']['images'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 32, 32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_map['cifar']['train']['images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-15 21:48:28.891313: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 614400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions 150000 and 50000 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8921/304761117.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m cifar_train_dataset = tf.data.Dataset.from_tensor_slices((\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdatasets_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cifar'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdatasets_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cifar'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ))\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    758\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \"\"\"\n\u001b[0;32m--> 760\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3327\u001b[0m         self._tensors[0].get_shape()[0]))\n\u001b[1;32m   3328\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3329\u001b[0;31m       batch_dim.assert_is_compatible_with(tensor_shape.Dimension(\n\u001b[0m\u001b[1;32m   3330\u001b[0m           tensor_shape.dimension_value(t.get_shape()[0])))\n\u001b[1;32m   3331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \"\"\"\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m       raise ValueError(\"Dimensions %s and %s are not compatible\" %\n\u001b[0m\u001b[1;32m    289\u001b[0m                        (self, other))\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 150000 and 50000 are not compatible"
     ]
    }
   ],
   "source": [
    "cifar_train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    datasets_map['cifar']['train']['images'],\n",
    "    datasets_map['cifar']['train']['labels']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorDataset shapes: ((150000, 32, 32), (50000,)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-encodeur\n",
    "\n",
    "Nous commençons par un auto-encodeur en TensorFlow à l'aide d'une classe personnalisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notre bloc encodeur\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_size=8, *args, **kwargs):\n",
    "        super(Encoder, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        # Couches\n",
    "        self.conv1 = layers.Conv2D(filters=4, kernel_size=3, strides=1, padding=\"SAME\", use_bias=True)\n",
    "        self.conv2 = layers.Conv2D(filters=1, kernel_size=3, strides=1, padding=\"SAME\", use_bias=True)\n",
    "        \n",
    "        # Activations\n",
    "        self.relu = layers.ReLu()\n",
    "        \n",
    "        # Couche latente\n",
    "        self.dense = layers.Dense(units=128, use_bias=True)\n",
    "        self.latent = layers.Dense(units=latent_size, use_bias=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.relu(x)\n",
    "        return self.latent(x)\n",
    "\n",
    "# Notre bloc décodeur\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Decoder, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        # Couches\n",
    "        self.de_conv1 = layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding=\"SAME\", use_bias=True)\n",
    "        self.de_conv2 = layers.Conv2DTranspose(filters=4, kernel_size=3, strides=1, padding=\"SAME\", use_bias=True)\n",
    "        \n",
    "        # Activations\n",
    "        self.relu = layers.ReLu()\n",
    "        \n",
    "        # Couche latente\n",
    "        self.dense = layers.Dense(units=784, use_bias=True)\n",
    "        self.de_latent = layers.Dense(units=128, use_bias=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.de_latent(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.de_conv2(x)\n",
    "        x = self.relu(x)\n",
    "        return self.de_conv1(x)\n",
    "\n",
    "\n",
    "# Classe finale : combinaison encodeur - decodeur\n",
    "class ConvolutionalAE(tf.keras.Model):\n",
    "    def __init__(self, latent_size=8, *args, **kwargs):\n",
    "        super(ConvolutionalAE, self).__init__(*args, **kwargs)\n",
    "        self.encoder = Encoder(latent_size=latent_size)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "model = ConvolutionalAE(latent_size=16)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant ce qu'il serait bien, ce serait de :\n",
    "- Développer la fonction d'entrainement (les calculs réalisés lors d'une époque)\n",
    "- Il semblerait qu'avec **tf.GradientTape()** nous avons la construction du graph avec les opérations effectuées durant le run\n",
    "- Construire nos jeux de données séparément avec **tf.data.Dataset**\n",
    "- Somme toute : continuer de suivre le tutoriel : https://www.tensorflow.org/tutorials/quickstart/advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
